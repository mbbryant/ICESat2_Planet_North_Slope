{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c443e12c-b62e-4afd-a72a-e18e251a2b9d",
   "metadata": {},
   "source": [
    "This notebook walks through the process of identifying shoreline from Planet imagery using Otsu thresholding of the normalized differenced water index (NDWI), as was done for the following publication: https://doi.org/10.5194/egusphere-2024-1656 . A few notes:\n",
    "\n",
    "- The user will need to provide their own imagery\n",
    "- This code requires matplotlib version 3.7.2 or lower\n",
    "- The process is not fully automated. The user will need to manually iterate through images and select the appropriate countour for each one\n",
    "- Planet Scences are sometimes delivered as mutiple files. These will need to be merged and the merged image added to the image file directory prior to running this notebook. We used the QGIS merge function to merge a scene from  \n",
    "25 June 2019.\n",
    "  \n",
    "contact: m1bryant@ucsd.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a2480d-bbc2-42ee-a51a-046a97b66b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.transform import from_gcps\n",
    "from rasterio.control import GroundControlPoint as GCP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #IMPORTANT should be version 3.7.2 or lower\n",
    "import pyproj\n",
    "from matplotlib import cm\n",
    "from affine import Affine\n",
    "import os\n",
    "import glob\n",
    "from shapely import wkt\n",
    "import datetime\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "783903a4-94c4-42f2-94d8-b64645ed91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions:\n",
    "def trim_image(A, X, Y, xs, ys):\n",
    "    #A: 3D array containing image data\n",
    "    #X, Y: 2D arrays of full image coordinates\n",
    "   #xs, ys: tuples containing desired bounds for trimming\n",
    "    x = X[0, :]\n",
    "    y = Y[:, 0]\n",
    "    x_t = x[(x >= xs[0]) & (x <= xs[1])] \n",
    "    y_t = y[(y >= ys[0]) & (y <= ys[1])]\n",
    " \n",
    "    x1 = np.where(x == x_t[0])[0][0]\n",
    "    x2 = np.where(x == x_t[len(x_t)-1])[0][0]\n",
    "    y2 = np.where(y == y_t[len(y_t)-1])[0][0]\n",
    "    y1 = np.where(y == y_t[0])[0][0]\n",
    "    A_small = A[:, y1:y2, x1:x2]\n",
    "    X_small = X[y1:y2, x1:x2]\n",
    "    Y_small = Y[y1:y2, x1:x2]\n",
    "    return A_small, X_small, Y_small\n",
    "\n",
    "def read_file(image_file, lons= None, lats = None):\n",
    "    with rasterio.open(image_file) as f:\n",
    "        T0 = f.transform  # upper-left pixel corner affine transform\n",
    "        p = (f.crs) #projection (UTM in this case)\n",
    "        A = f.read()\n",
    "    n_rows = A.shape[1]\n",
    "    n_cols = A.shape[2]\n",
    "# Get affine transform for pixel centres\n",
    "    T1 = T0 * Affine.translation(0.5, 0.5)\n",
    "\n",
    "# convert row/column index to eastings and norhtings\n",
    "\n",
    "    min_coords = T1*(0, 0)\n",
    "    max_coords = T1*(n_cols, n_rows)\n",
    "    x_min = min_coords[0]\n",
    "    x_max = max_coords[0]\n",
    "    y_min = min_coords[1]\n",
    "    y_max = max_coords[1]\n",
    "    x = np.linspace(x_min, x_max, n_cols)\n",
    "    y = np.linspace(y_min, y_max, n_rows)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    if lons != None:\n",
    "        A, X, Y = trim_image(A, X, Y, lons, lats)\n",
    "    return A,p, X, Y \n",
    "\n",
    "def make_rgb(A):\n",
    "    #A: 3D array containing image data\n",
    "    blue = np.float64(A[0, :, :])\n",
    "    green = np.float64(A[1, :, :])\n",
    "    red = np.float64(A[2, :, :])\n",
    "    \n",
    "    \n",
    "    #re-assign very bringt (snow/ice) pixels\n",
    "    blue[np.where(blue > 3000)] = 3000\n",
    "    red[np.where(red > 3000)] = 3000\n",
    "    green[np.where(green > 3000)] = 3000\n",
    "    #contrast enhancement\n",
    "    red_n = ((red - np.min(red[red >0]))*1/(np.max(red) - np.min(red[red >0])))\n",
    "    blue_n = ((blue - np.min(blue[blue > 0]))*1/(np.max(blue) - np.min(blue[blue>0])))\n",
    "    green_n = ((green - np.min(green[green > 0]))*1/(np.max(green) - np.min(green[green > 0])))\n",
    "\n",
    "\n",
    "\n",
    "    rgb_image = np.stack([red_n, green_n, blue_n], axis = 2).reshape((red.shape[0], red.shape[1], 3))\n",
    "    return rgb_image\n",
    "\n",
    "def get_ndwi(A):\n",
    "    green = np.float64(A[1, :, :])\n",
    "    ir = np.float64(A[3, :, :])\n",
    "    ndwi = (ir-green)/(ir+green)\n",
    "    return ndwi\n",
    "\n",
    "def otsu_thresholding(img, n_bins):\n",
    "    hist_n, bins= np.histogram(img[~np.isnan(img)], n_bins, density = True) #normalized histogram\n",
    "    hist, bins= np.histogram(img[~np.isnan(img)], n_bins) #histogram with counts\n",
    "    t = 0 #arbitrrily starting threshold\n",
    "    var = 1e-6 #arbitrailiy small starting variance\n",
    "    var_list = []\n",
    "    for b in np.arange(len(bins)-1):\n",
    "        t_new = bins[b+1] #right edge of highest bin (our potential threshold value)\n",
    "        hist_1 = hist_n[0:b]\n",
    "        bins_1 = bins[1:b+1]\n",
    "        bins_2 = bins[b+1:len(bins)]\n",
    "        hist_2 = hist_n[b:len(hist_n)]\n",
    "        q1 = np.sum(hist_1) #sum of probabilities below threshold\n",
    "        q2 = np.sum(hist_2) #sum of probabilities above threshold\n",
    "        #get means and variancesof the two catgories\n",
    "        mu_1 = np.sum(hist_1*bins_1/q1)\n",
    "        mu_2 = np.sum(hist_2*bins_2/q2)\n",
    "    \n",
    "        #calculate weighted between-class variance (we want to maximize this value)\n",
    "        var_t = (q1*q2)*(mu_1-mu_2)**2\n",
    "        var_list.append(var_t)\n",
    "        if var_t > var:\n",
    "            t = t_new\n",
    "            var = var_t \n",
    "    return t\n",
    "\n",
    "def date_from_fname(file): \n",
    "    datestring = file[0:8]\n",
    "    return datestring\n",
    "\n",
    "def id_from_fname(file):\n",
    "    segments = file.split('_')\n",
    "    f_id = [s for s in segments if len(s)==4][0]\n",
    "    return f_id\n",
    "\n",
    "\n",
    "def get_contours(file, lons, lats):\n",
    "    A,p, X, Y = read_file(file, lons, lats)\n",
    "    rgb = make_rgb(A)\n",
    "    \n",
    "    fig_hist = plt.figure()\n",
    "    image = get_ndwi(A)\n",
    "    t = otsu_thresholding(image, 100)\n",
    "    plt.hist(image.flatten(), bins = 100)\n",
    "    plt.xlabel('NDWI', fontsize = 15)\n",
    "    plt.ylabel('Frequency', fontsize = 15)\n",
    "    plt.axvline(t, c = 'k', ls = '--')\n",
    "    \n",
    "    fig_0 = plt.figure()\n",
    "    c = plt.contour(X, Y, image, levels = [t])\n",
    "    plt.close(fig_0)\n",
    "    contours = c.allsegs[0]\n",
    "    #sort by length\n",
    "    l = [int(len(i)) for i in contours]\n",
    "    l_s = [int(len(i)) for i in contours]\n",
    "    l_s.sort(reverse=True)\n",
    "    indices = l_s[0:10]\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(2, 1, 1)\n",
    "    ax1.imshow(rgb, extent = (np.min(X), np.max(X), np.min(Y), np.max(Y)))\n",
    "    ax2 = fig.add_subplot(2, 1, 2)\n",
    "    ax2.imshow(image, extent = (np.min(X), np.max(X), np.min(Y), np.max(Y)), cmap = cm.gray)\n",
    "    contour_list = [contours[np.where(l == np.int64(i))[0][0]] for i in indices][0:10] #save the 10 longest\n",
    "    for i in range(0, len(contour_list)):\n",
    "        ax1.plot(contour_list[i][:, 0], contour_list[i][:, 1], c = cm.tab10(i))#c = cm.tab10(i)\n",
    "        ax2.plot(contour_list[i][:, 0], contour_list[i][:, 1], c = 'tab:blue')\n",
    "    sm = plt.cm.ScalarMappable(cmap=cm.tab10, norm=plt.Normalize(vmin=0, vmax=10))\n",
    "    plt.colorbar(sm, ax = ax1)\n",
    "    plt.colorbar(sm, ax = ax2)\n",
    "    \n",
    "    return contour_list\n",
    "\n",
    "def save_countour(output_dir, contour, date, fid):\n",
    "    os.chdir(output_dir)\n",
    "    np.savetxt(\"coastline_\" + date + '_' + f_id + '_ndwi' + '.csv', contour, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38971f22-1b33-41eb-ab28-29f9ed286a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/Planet Images' #replace with appropriate data directory\n",
    "os.chdir(data_dir)\n",
    "\n",
    "##bbox for study area\n",
    "lons = [469400, 477700]\n",
    "\n",
    "lats = [7864300, 7865200]\n",
    "\n",
    "output_dir = '/Users/m1bryant/Documents/Python/tc_paper_code/data/coastlines/all_coastlines' #replace with appropriate directory\n",
    "files = glob.glob('*SR_clip.tif')\n",
    "files.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b994057-e153-417d-81ba-e8a9ab9c71ff",
   "metadata": {},
   "source": [
    "# Iterating through files \n",
    "\n",
    "The next 2 code blocks provide code to run outsu thresholding on an image, plot and label all of the contours that could be a shoreline, and save the specified contour. In order to iterate through all imagery and save successful coastline aquisitions:\n",
    "\n",
    "- increase 'file_num' to go to the next image and run the first cell \n",
    "- Inspect the output. The legend on the contour plot shows the index label for each contour\n",
    "- If you want to save one of the plotted contours, set  'c' in the second cell to the corresponding index and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ca8d9d4-c08b-4342-b463-d77feda2e685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, '20190625 1006')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib auto\n",
    "os.chdir(data_dir)\n",
    "file_num = 3 #update this number to iterate through directory\n",
    "file = files[file_num]\n",
    "date = date_from_fname(file)\n",
    "f_id = id_from_fname(file)\n",
    "contour_list =  get_contours(file, lons, lats)    \n",
    "plt.suptitle(date + ' ' + f_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d043249-06be-4d17-92fc-f2b90a63b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "\n",
    "save_countour(output_dir, contour_list[c], date, f_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244eccc9-5caa-4333-a301-88677df6ca43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
